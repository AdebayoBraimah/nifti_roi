{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global variable(s)\n",
    "# scripts_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "scripts_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Command():\n",
    "    '''\n",
    "    Creates a command and an empty command list for UNIX command line programs/applications. Primary use and\n",
    "    use-cases are intended for the subprocess module and its associated classes (i.e. call/run).\n",
    "    Attributes:\n",
    "        command: Command to be performed on the command line\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Init doc-string for Command class.\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def init_cmd(self, command):\n",
    "        '''\n",
    "        Init command function for initializing commands to be used on UNIX command line.\n",
    "        \n",
    "        Arguments:\n",
    "            command (string): Command to be used. Note: command used must be in system path\n",
    "        Returns:\n",
    "            cmd_list (list): Mutable list that can be appended to.\n",
    "        '''\n",
    "        self.command = command\n",
    "        self.cmd_list = [f\"{self.command}\"]\n",
    "        return self.cmd_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(cmd_list,stdout=\"\",stderr=\"\"):\n",
    "    '''\n",
    "    Uses python's built-in subprocess class to run a command from an input command list.\n",
    "    The standard output and error can optionally be written to file.\n",
    "    \n",
    "    Arguments:\n",
    "        cmd_list(list): Input command list to be run from the UNIX command line.\n",
    "        stdout(file): Output file to write standard output to.\n",
    "        stderr(file): Output file to write standard error to.\n",
    "    Returns:\n",
    "        stdout(file): Output file that contains the standard output.\n",
    "        stderr(file): Output file that contains the standard error.\n",
    "    '''\n",
    "    if stdout and stderr:\n",
    "        with open(stdout,\"w\") as file:\n",
    "            with open(stderr,\"w\") as file_err:\n",
    "                subprocess.call(cmd_list,stdout=file,stderr=file_err)\n",
    "                file.close(); file_err.close()\n",
    "    elif stdout:\n",
    "        with open(stdout,\"w\") as file:\n",
    "            subprocess.call(cmd_list,stdout=file)\n",
    "            file.close()\n",
    "        stderr = None\n",
    "    else:\n",
    "        subprocess.call(cmd_list)\n",
    "        stdout = None\n",
    "        stderr = None\n",
    "\n",
    "    return stdout,stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roi_loc(coords,vol_atlas_num=3):\n",
    "    '''\n",
    "    Uses input list of X,Y,Z MNI space mm coordinates to identify ROIs.\n",
    "    \n",
    "    NOTE: External bash script is used.\n",
    "    \n",
    "    Arguments:\n",
    "        coords(list): Coordinate list with a lenth of 3 that corresponds to the XYZ coordinates of some ROI in MNI space.\n",
    "        vol_atlas_num(int): Atlas to be used in FSL's `atlasquery`. Number corresponds to an atlas. See FSL's `atlasquery` help menu for details.\n",
    "    Returns:\n",
    "        roi_list(list): List of ROIs generated from input coordinates.\n",
    "    '''\n",
    "    \n",
    "    # Define volume atlas number dictionary\n",
    "    vol_atlas_dict = {\n",
    "    1: \"Cerebellar Atlas in MNI152 space after normalization with FLIRT\",\n",
    "    2: \"Cerebellar Atlas in MNI152 space after normalization with FNIRT\",\n",
    "    3: \"Harvard-Oxford Cortical Structural Atlas\",\n",
    "    4: \"Harvard-Oxford Subcortical Structural Atlas\",\n",
    "    5: \"Human Sensorimotor Tracts Labels\",\n",
    "    6: \"JHU ICBM-DTI-81 White-Matter Labels\",\n",
    "    7: \"JHU White-Matter Tractography Atlas\",\n",
    "    8: \"Juelich Histological Atlas\",\n",
    "    9: \"MNI Structural Atlas\",\n",
    "    10: \"Mars Parietal connectivity-based parcellation\",\n",
    "    11: \"Mars TPJ connectivity-based parcellation\",\n",
    "    12: \"Neubert Ventral Frontal connectivity-based parcellation\",\n",
    "    13: \"Oxford Thalamic Connectivity Probability Atlas\",\n",
    "    14: \"Oxford-Imanova Striatal Connectivity Atlas 3 sub-regions\",\n",
    "    15: \"Oxford-Imanova Striatal Connectivity Atlas 7 sub-regions\",\n",
    "    16: \"Oxford-Imanova Striatal Structural Atlas\",\n",
    "    17: \"Sallet Dorsal Frontal connectivity-based parcellation\",\n",
    "    18: \"Subthalamic Nucleus Atlas\",\n",
    "    19: \"Talairach Daemon Labels\"}\n",
    "    \n",
    "    # Define list and output file\n",
    "    roi_list = list()\n",
    "    out_file = \"subcort.rois.txt\"\n",
    "    \n",
    "    if len(coords) == 3:\n",
    "        atlasq_cmd = os.path.join(scripts_dir,\"atlasq.sh\")\n",
    "        atlasq = Command().init_cmd(atlasq_cmd)\n",
    "        atlasq.append(f\"--coord\")\n",
    "        atlasq.append(f\"\\\"{coords[0]},{coords[1]},{coords[2]}\\\"\")\n",
    "        atlasq.append(\"--atlas-num\")\n",
    "        atlasq.append(f\"{vol_atlas_num}\")\n",
    "    \n",
    "        run(atlasq,out_file)\n",
    "\n",
    "        with open(out_file,\"r\") as file:\n",
    "            text = file.readlines()\n",
    "            for i in range(0,len(text)):\n",
    "                text[i] = re.sub(f\"<b>{vol_atlas_dict[vol_atlas_num]}</b><br>\",\"\",text[i].rstrip())\n",
    "\n",
    "        os.remove(out_file)\n",
    "        \n",
    "        if len(text) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            roi_list.extend(text) \n",
    "        \n",
    "    return roi_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vol_clust(nii_file,thresh=0.95,dist=0,vol_atlas_num=3):\n",
    "    '''\n",
    "    Identifies clusters in a volumetric (NIFTI) file.\n",
    "    \n",
    "    Arguments:\n",
    "        nii_file(file): Input NIFTI file\n",
    "        thresh(float): Cluster minimum threshold\n",
    "        dist(float): Minimum distance between clusters\n",
    "        vol_atlas_num(int): Atlas to be used in FSL's `atlasquery`. Number corresponds to an atlas. See FSL's `atlasquery` help menu for details.\n",
    "    Returns:\n",
    "        roi_list(list): List of ROIs that overlap with some given cluster\n",
    "    '''\n",
    "    \n",
    "    out_file = \"vol.cluster.tsv\"\n",
    "    \n",
    "    roi_list = list()\n",
    "    tmp_list = list()\n",
    "    \n",
    "    vol_clust = Command().init_cmd(\"cluster\")\n",
    "    \n",
    "    vol_clust.append(f\"--in={nii_file}\")\n",
    "    vol_clust.append(f\"--thresh={thresh}\")\n",
    "    vol_clust.append(f\"--peakdist={dist}\")\n",
    "    vol_clust.append(\"--mm\")\n",
    "    \n",
    "    run(vol_clust,out_file)\n",
    "    \n",
    "    df_tmp = pd.read_csv(out_file,sep=\"\\t\")\n",
    "    \n",
    "    os.remove(out_file)\n",
    "    \n",
    "    df = df_tmp[['MAX X (mm)','MAX Y (mm)','MAX Z (mm)']].copy()\n",
    "    \n",
    "    for i in range(0,len(df)):\n",
    "        coord_list=[df['MAX X (mm)'][i],df['MAX Y (mm)'][i],df['MAX Z (mm)'][i]]\n",
    "        tmp_list = roi_loc(coord_list,vol_atlas_num)\n",
    "        if len(tmp_list) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            roi_list.extend(tmp_list)\n",
    "    \n",
    "    return roi_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_atlas_file(atlas_info):\n",
    "    '''\n",
    "    Reads CSV of key, value pairs of enumerated ROIs for some corresponding atlas.\n",
    "    \n",
    "    Arguments:\n",
    "        atlas_info(file): Input CSV file of enumerated ROI key, value pairs\n",
    "    Returns:\n",
    "        atlas_dict(dict): Dictionary of atlas key, value pairs\n",
    "    '''\n",
    "    \n",
    "    atlas_info = os.path.abspath(atlas_info)\n",
    "    df = pd.read_csv(atlas_info,header=None); df.columns = ['key', 'id']\n",
    "    atlas_dict = df.set_index('key').to_dict(orient='dict')['id']\n",
    "    \n",
    "    return atlas_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/smart/Desktop/vol_cluster/test.files/Atlas_ROIs.2.csv'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atlas_info = os.path.abspath(atlas_info)\n",
    "atlas_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas_info = \"test.files/infant-neo-aal.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '?????',\n",
       " 1: 'Precentral_L',\n",
       " 2: 'Precentral_R',\n",
       " 3: 'Frontal_Sup_L',\n",
       " 4: 'Frontal_Sup_R',\n",
       " 5: 'Frontal_Sup_Orb_L',\n",
       " 6: 'Frontal_Sup_Orb_R',\n",
       " 7: 'Frontal_Mid_L',\n",
       " 8: 'Frontal_Mid_R',\n",
       " 9: 'Frontal_Mid_Orb_L',\n",
       " 10: 'Frontal_Mid_Orb_R',\n",
       " 11: 'Frontal_Inf_Oper_L',\n",
       " 12: 'Frontal_Inf_Oper_R',\n",
       " 13: 'Frontal_Inf_Tri_L',\n",
       " 14: 'Frontal_Inf_Tri_R',\n",
       " 15: 'Frontal_Inf_Orb_L',\n",
       " 16: 'Frontal_Inf_Orb_R',\n",
       " 17: 'Rolandic_Oper_L',\n",
       " 18: 'Rolandic_Oper_R',\n",
       " 19: 'Supp_Motor_Area_L',\n",
       " 20: 'Supp_Motor_Area_R',\n",
       " 21: 'Olfactory_L',\n",
       " 22: 'Olfactory_R',\n",
       " 23: 'Frontal_Sup_Medial_L',\n",
       " 24: 'Frontal_Sup_Medial_R',\n",
       " 25: 'Frontal_Med_Orb_L',\n",
       " 26: 'Frontal_Med_Orb_R',\n",
       " 27: 'Rectus_L',\n",
       " 28: 'Rectus_R',\n",
       " 29: 'Insula_L',\n",
       " 30: 'Insula_R',\n",
       " 31: 'Cingulum_Ant_L',\n",
       " 32: 'Cingulum_Ant_R',\n",
       " 33: 'Cingulum_Mid_L',\n",
       " 34: 'Cingulum_Mid_R',\n",
       " 35: 'Cingulum_Post_L',\n",
       " 36: 'Cingulum_Post_R',\n",
       " 37: 'Hippocampus_L',\n",
       " 38: 'Hippocampus_R',\n",
       " 39: 'ParaHippocampal_L',\n",
       " 40: 'ParaHippocampal_R',\n",
       " 41: 'Amygdala_L',\n",
       " 42: 'Amygdala_R',\n",
       " 43: 'Calcarine_L',\n",
       " 44: 'Calcarine_R',\n",
       " 45: 'Cuneus_L',\n",
       " 46: 'Cuneus_R',\n",
       " 47: 'Lingual_L',\n",
       " 48: 'Lingual_R',\n",
       " 49: 'Occipital_Sup_L',\n",
       " 50: 'Occipital_Sup_R',\n",
       " 51: 'Occipital_Mid_L',\n",
       " 52: 'Occipital_Mid_R',\n",
       " 53: 'Occipital_Inf_L',\n",
       " 54: 'Occipital_Inf_R',\n",
       " 55: 'Fusiform_L',\n",
       " 56: 'Fusiform_R',\n",
       " 57: 'Postcentral_L',\n",
       " 58: 'Postcentral_R',\n",
       " 59: 'Parietal_Sup_L',\n",
       " 60: 'Parietal_Sup_R',\n",
       " 61: 'Parietal_Inf_L',\n",
       " 62: 'Parietal_Inf_R',\n",
       " 63: 'SupraMarginal_L',\n",
       " 64: 'SupraMarginal_R',\n",
       " 65: 'Angular_L',\n",
       " 66: 'Angular_R',\n",
       " 67: 'Precuneus_L',\n",
       " 68: 'Precuneus_R',\n",
       " 69: 'Paracentral_Lobule_L',\n",
       " 70: 'Paracentral_Lobule_R',\n",
       " 71: 'Caudate_L',\n",
       " 72: 'Caudate_R',\n",
       " 73: 'Putamen_L',\n",
       " 74: 'Putamen_R',\n",
       " 75: 'Pallidum_L',\n",
       " 76: 'Pallidum_R',\n",
       " 77: 'Thalamus_L',\n",
       " 78: 'Thalamus_R',\n",
       " 79: 'Heschl_L',\n",
       " 80: 'Heschl_R',\n",
       " 81: 'Temporal_Sup_L',\n",
       " 82: 'Temporal_Sup_R',\n",
       " 83: 'Temporal_Pole_Sup_L',\n",
       " 84: 'Temporal_Pole_Sup_R',\n",
       " 85: 'Temporal_Mid_L',\n",
       " 86: 'Temporal_Mid_R',\n",
       " 87: 'Temporal_Pole_Mid_L',\n",
       " 88: 'Temporal_Pole_Mid_R',\n",
       " 89: 'Temporal_Inf_L',\n",
       " 90: 'Temporal_Inf_R '}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_atlas_file(atlas_info=atlas_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ext(file):\n",
    "    '''\n",
    "    Removes extension of some input file string. Primarily intended for NIFTI files.\n",
    "    \n",
    "    Arguments:\n",
    "        file(str): Input file name string\n",
    "    Returns:\n",
    "        name(str): File name string with extension removed\n",
    "    '''\n",
    "    \n",
    "    if '.nii.gz' in file:\n",
    "        name = file[:-7]\n",
    "    elif '.nii' in file:\n",
    "        name = file[:-4]\n",
    "    elif '.txt' in file or '.tsv' in file or '.csv' in file:\n",
    "        name = file\n",
    "    else:\n",
    "        name = file\n",
    "    \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_img_dtype(img,data_type=\"int\"):\n",
    "    '''\n",
    "    Converts image data type to some other arbitrary data type. See fslmaths help menu for further details.\n",
    "    \n",
    "    Arguments:\n",
    "        img(NIFTI file): Input NIFTI file\n",
    "        data_type(str): Output data type (e.g. 'int','float','char','float','double')\n",
    "    Returns:\n",
    "        out_file(NIFTI file): NIFTI file with desired data type\n",
    "    '''\n",
    "    \n",
    "    img = os.path.abspath(img)\n",
    "    out_dir = os.path.dirname(img)\n",
    "    \n",
    "    img = remove_ext(img)\n",
    "    name = os.path.basename(img)\n",
    "    \n",
    "    out_file = os.path.join(out_dir,name + \".int\" + \".nii.gz\")\n",
    "    \n",
    "    img_dtype = Command().init_cmd(\"fslmaths\")\n",
    "    img_dtype.append(\"-dt\")\n",
    "    img_dtype.append(data_type)\n",
    "    img_dtype.append(img)\n",
    "    img_dtype.append(out_file)\n",
    "    img_dtype.append(\"-odt\")\n",
    "    img_dtype.append(data_type)\n",
    "    \n",
    "    run(img_dtype)\n",
    "    \n",
    "    return out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_atlas_data(nii_atlas,atlas_info,data_type=\"int\"):\n",
    "    '''\n",
    "    Loads atlas data from input NIFTI neuroimage atlas and it's corresponding\n",
    "    enumerated key, value paired atlas CSV file.\n",
    "    \n",
    "    Arguments:\n",
    "        nii_atlas(NIFTI file): Input NIFTI atlas\n",
    "        atlas_info(file): Corresponding atlas CSV file\n",
    "        data_type(str): Output data type (e.g. 'int','float','char','float','double')\n",
    "    Returns:\n",
    "        atlas_data(numpy array): Atlas data represented as an N x M x P array\n",
    "        atlas_dict(dict): Atlas dictionary of key, value pairs\n",
    "    '''\n",
    "    \n",
    "    # Load atlas key/ID information\n",
    "    atlas_dict = read_atlas_file(atlas_info)\n",
    "    \n",
    "    # Convert input image from float to int\n",
    "    int_data = convert_img_dtype(nii_atlas,data_type)\n",
    "    \n",
    "    # Load/export data as numpy array\n",
    "    img = nib.load(int_data)\n",
    "    atlas_data = img.get_fdata()\n",
    "    \n",
    "    # Clean-up\n",
    "    os.remove(int_data)\n",
    "    \n",
    "    return atlas_data,atlas_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cluster_vol(nii_file,thresh=0.95,dist=0):\n",
    "    '''\n",
    "    Creates enumerated clusters from input NIFTI file and writes the enumerated clusters to a separate NIFTI volume.\n",
    "    \n",
    "    Arguments:\n",
    "        nii_file(NIFTI file): Input NIFTI file\n",
    "        thresh(float): Minimum threshold \n",
    "        dist(float): Minimum distance between clusters\n",
    "    Returns:\n",
    "        out_file(NIFTI file): Output NIFTI file of enumerated clusters\n",
    "        out_stat(file): Corresponding table of enumerated clusters with MNI space mm and vox coordinates\n",
    "    '''\n",
    "    \n",
    "    # Construct file paths\n",
    "    nii_file = os.path.abspath(nii_file)\n",
    "    out_dir = os.path.dirname(nii_file)\n",
    "    \n",
    "    nii_file = remove_ext(nii_file)\n",
    "    name = os.path.basename(nii_file)\n",
    "    out_prefix = os.path.join(out_dir,name + \".cluster\")\n",
    "    \n",
    "    out_file = out_prefix + \".nii.gz\"\n",
    "    out_stat = out_prefix + \".txt\"\n",
    "    \n",
    "    # Make cluster(s) from input data\n",
    "    clust_cmd = Command().init_cmd(\"cluster\")\n",
    "    clust_cmd.append(f\"--in={nii_file}\")\n",
    "    clust_cmd.append(f\"--thresh={thresh}\")\n",
    "    clust_cmd.append(f\"--peakdist={dist}\")\n",
    "    clust_cmd.append(f\"--oindex={out_file}\")\n",
    "    # clust_cmd.append(f\"--othresh={out_file}\")\n",
    "    \n",
    "    run(clust_cmd,out_stat)\n",
    "    \n",
    "    return out_file,out_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nii_vol(nii_file,thresh=0.95,dist=0):\n",
    "    '''\n",
    "    Reads in NIFTI volume information, creates a volume of enumerated clusters, and then stores \n",
    "    those clusters in an N x M x P array.\n",
    "    \n",
    "    Arguments:\n",
    "        nii_file(NIFTI file): Input NIFTI file\n",
    "        thresh(float): Minimum threshold\n",
    "        dist(float): Minimum distance between clusters\n",
    "    Returns:\n",
    "        img_data(numpy array): N x M x P numpy array of the clusters\n",
    "    '''\n",
    "    \n",
    "    # Create volume clusters\n",
    "    [clust_file,clust_stat] = make_cluster_vol(nii_file,thresh,dist)\n",
    "    \n",
    "    # Load/export data as numpy array\n",
    "    img = nib.load(clust_file)\n",
    "    img_data = img.get_fdata()\n",
    "    \n",
    "    # Clean-up\n",
    "    os.remove(clust_file)\n",
    "    os.remove(clust_stat)\n",
    "    \n",
    "    return img_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roi_name(cluster_data,atlas_data,atlas_dict):\n",
    "    '''\n",
    "    Finds ROI names from overlapping clusters in a NIFTI volume by voxel matching.\n",
    "    \n",
    "    Arguments:\n",
    "        cluster_data(numpy array): Input numpy of data\n",
    "        atlas_data(numpy array): Numpy array of labeled surface vertices for some specific hemisphere\n",
    "        atlas_dict(dict): Dictionary of label IDs to ROI names\n",
    "    Returns:\n",
    "        roi_list(list): List of ROIs overlapped by cluster(s)\n",
    "    '''\n",
    "    \n",
    "    # Flatten matrices\n",
    "    cluster_data = cluster_data.flatten(order='C')\n",
    "    atlas_data = atlas_data.flatten(order='C')\n",
    "    \n",
    "    tmp_list = list()\n",
    "    roi_list = list()\n",
    "    \n",
    "    for idx,val in enumerate(cluster_data):\n",
    "        if cluster_data[idx] == 0:\n",
    "            atlas_data[idx] = 0\n",
    "            \n",
    "    for i in np.unique(atlas_data)[1:]:\n",
    "        tmp_list = atlas_dict[i]\n",
    "        roi_list.append(tmp_list)\n",
    "        \n",
    "    return roi_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_spread(file,out_file,roi_list):\n",
    "    '''\n",
    "    Writes the contents or roi_list to a spreadsheet.\n",
    "    \n",
    "    Arguments:\n",
    "        file (file): Input CIFTI file\n",
    "        out_file (file): Output csv file name and path. This file need not exist at runtime.\n",
    "        roi_list(list): List of ROIs to write to file\n",
    "    Returns: \n",
    "        out_file (csv file): Output csv file name and path.\n",
    "    '''\n",
    "    \n",
    "    # Strip csv file extension from output file name\n",
    "    if '.csv' in out_file:\n",
    "        out_file = os.path.splitext(out_file)[0]\n",
    "        out_file = out_file + '.csv'\n",
    "    elif '.tsv' in out_file:\n",
    "        out_file = os.path.splitext(out_file)[0]\n",
    "        out_file = out_file + '.csv'\n",
    "    elif '.txt' in out_file:\n",
    "        out_file = os.path.splitext(out_file)[0]\n",
    "        out_file = out_file + '.csv'\n",
    "    else:\n",
    "        out_file = out_file + '.csv'\n",
    "    \n",
    "    # Construct image dictionary\n",
    "    file = os.path.abspath(file)\n",
    "    img_dict = {\"File\":file,\n",
    "         \"ROIs\":[roi_list]}\n",
    "    \n",
    "    # Create dataframe from image dictionary\n",
    "    df = pd.DataFrame.from_dict(img_dict,orient='columns')\n",
    "    \n",
    "    # Write output CSV file\n",
    "    if os.path.exists(out_file):\n",
    "        df.to_csv(out_file, sep=\",\", header=False, index=False, mode='a')\n",
    "    else:\n",
    "        df.to_csv(out_file, sep=\",\", header=True, index=False, mode='w')\n",
    "    \n",
    "    return out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_vol(nii_file,out_file,thresh = 0.95, dist = 0, vol_atlas_num = 3, nii_atlas = \"\", atlas_info = \"\"):\n",
    "    '''\n",
    "    Identifies ROIs that have overlap with some cluster(s) from the input NIFTI file.\n",
    "    \n",
    "    Arguments:\n",
    "        nii_file(NIFTI file): Input NIFTI volume file\n",
    "        out_file(file): Name for output CSV\n",
    "        thresh(float): Threshold values below this value\n",
    "        dist(float): Minimum distance between two or more clusters\n",
    "        vol_atlas_num(int): Atlas to be used in FSL's `atlasquery`. Number corresponds to an atlas. See FSL's `atlasquery` help menu for details.\n",
    "        nii_atlas(NIFTI file): NIFTI atlas file\n",
    "        atlas_info(file): Corresponding CSV key, value pairs of ROIs for atlas file\n",
    "    Returns:\n",
    "      out_filefile(file): Output CSV file\n",
    "    '''\n",
    "    \n",
    "    if nii_atlas and atlas_info:\n",
    "        # Read atlas data and info\n",
    "        [atlas_data,atlas_dict] = load_atlas_data(nii_atlas,atlas_info)\n",
    "\n",
    "        # Read NIFTI data and find clusters\n",
    "        img_data = load_nii_vol(nii_file,thresh,dist)\n",
    "\n",
    "        # Identify cluster and ROI overlaps\n",
    "        roi_list = get_roi_name(img_data,atlas_data,atlas_dict)\n",
    "    else:\n",
    "        roi_list = vol_clust(nii_file,thresh,dist,vol_atlas_num)\n",
    "    \n",
    "    # Write spreadsheet to file\n",
    "    if len(roi_list) != 0:\n",
    "        out_file = write_spread(nii_file,out_file,roi_list)\n",
    "    \n",
    "    return out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nii_file = \"test.files/test.file.nii.gz\"\n",
    "nii_atlas = \"test.files/Atlas_ROIs.2.nii.gz\"\n",
    "atlas_info = \"test.files/Atlas_ROIs.2.csv\"\n",
    "out_file = \"test.3.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 2 fields in line 7, saw 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-64a84f8d1a0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mproc_vol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnii_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnii_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.77\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvol_atlas_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnii_atlas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnii_atlas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matlas_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matlas_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-ee96fda5335f>\u001b[0m in \u001b[0;36mproc_vol\u001b[0;34m(nii_file, out_file, thresh, dist, vol_atlas_num, nii_atlas, atlas_info)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnii_atlas\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0matlas_info\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Read atlas data and info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0matlas_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0matlas_dict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_atlas_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnii_atlas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0matlas_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Read NIFTI data and find clusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-a4ea29cb44bd>\u001b[0m in \u001b[0;36mload_atlas_data\u001b[0;34m(nii_atlas, atlas_info, data_type)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Load atlas key/ID information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0matlas_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_atlas_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matlas_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Convert input image from float to int\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-9a1873f2003a>\u001b[0m in \u001b[0;36mread_atlas_file\u001b[0;34m(atlas_info)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0matlas_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matlas_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matlas_info\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'key'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0matlas_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'key'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nrows'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1993\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1995\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1996\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 2 fields in line 7, saw 3\n"
     ]
    }
   ],
   "source": [
    "proc_vol(nii_file=nii_file,out_file=out_file,thresh = 1.77, dist = 20, vol_atlas_num = 4, nii_atlas = nii_atlas, atlas_info = atlas_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
